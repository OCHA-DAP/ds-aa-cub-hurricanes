---
jupyter: ds-aa-cub-hurricanes
---

## Appendix 1A: Quick Check Monitor Quantile Calculations

The quantile calculations were refactored due to the conceptual change in logic between realtime monitoring and historical analysis. Additionally, there are different data sets being used for historical analysis and realtime monitoring. Therefore, this script is meant to sanity check the calculations used for real time monitoring.

### Background

During the development of the hurricane monitoring system, two different approaches emerged for calculating rainfall quantiles:

1. **Historical Analysis**: Uses pre-computed quantile values stored in `zma_stats_imerg_quantiles.parquet`, calculated from comprehensive historical storm data (2000-2024) using IBTrACS tracks.

2. **Real-time Monitoring**: Uses on-the-fly calculations via `IMERGRasterProcessor` with 0.8 quantile and 2-day rolling sums, applied to IMERG rainfall data during active storm monitoring.

### Validation Approach

This validation compares the two calculation methods to ensure consistency between historical baselines and real-time assessments. The test:

1. **Loads Historical Data**: Both pre-computed quantiles and IBTrACS storm tracks from the same time period (2000-2024)
2. **Applies Monitoring Criteria**: Filters storms using the same thresholds as real-time monitoring (wind speed ‚â•105 kt, ZMA intersection)
3. **Optimized Processing**: Uses a three-stage filter to efficiently identify qualifying storms:
   - Wind speed pre-filter (cheapest operation)
   - Boolean ZMA intersection check (medium cost)
   - Full track interpolation and analysis (most expensive, only for qualifying storms)
4. **Calculates On-the-fly Values**: Uses `IMERGRasterProcessor` to compute rainfall metrics for qualifying storms
5. **Compares Results**: Matches storms by SID and compares on-the-fly calculations with pre-computed values

### Testing Implementation

The validation framework includes several optimizations and debugging features:

- **Efficient Filtering**: Applies wind thresholds before expensive geometric operations to minimize processing time
- **Exact ID Matching**: Uses SID (IBTrACS storm identifier) for precise matching between datasets  
- **Debug Output**: Provides detailed logging of storm qualification process and date range validation
- **Error Handling**: Includes comprehensive exception handling with traceback for troubleshooting
- **Sample Limiting**: Processes a subset of storms for rapid validation during development

This ensures that real-time monitoring calculations produce results consistent with historical analysis, maintaining accuracy and reliability in operational hurricane monitoring. 



```{python}
import pandas as pd
import numpy as np
import ocha_stratus as stratus
from src.datasources import nhc, ibtracs
from src.monitoring.monitoring_utils import (
    CubaHurricaneMonitor,
    IMERGRasterProcessor,
)
from src.constants import PROJECT_PREFIX, THRESHS
```


```{python}
print("üîç Rainfall Aggregation Validation")
print("=" * 50)

# Load pre-computed aggregations from exploration
blob_name = (
    f"{PROJECT_PREFIX}/processed/storm_stats/zma_stats_imerg_quantiles.parquet"
)
print(f"üìä Loading pre-computed aggregations from: {blob_name}")
try:
    df_precomputed = stratus.load_parquet_from_blob(blob_name)
    print(f"‚úÖ Loaded {len(df_precomputed)} storms from pre-computed data")
    print(
        f"   Date range: {df_precomputed['valid_time_min'].min().date()} to {df_precomputed['valid_time_max'].max().date()}"
    )
except Exception as e:
    print(f"‚ùå Error loading pre-computed data: {e}")
    df_precomputed = None

# Load historical observational tracks from IBTrACS (same as used in imerg_aggregation.md)
print(f"\nüåÄ Loading historical observational tracks from IBTrACS...")
try:
    # Load the same historical data used to create the pre-computed aggregations
    ibtracs_blob_name = (
        f"{PROJECT_PREFIX}/processed/ibtracs/zma_tracks_2000-2024.parquet"
    )
    obsv_tracks = stratus.load_parquet_from_blob(ibtracs_blob_name)

    # Load storm metadata to get SID to ATCF mapping (following your approach)
    print("üìã Loading SID to ATCF mapping...")
    df_storms = ibtracs.load_storms()

    # Filter to relevant storms and get SID-ATCF mapping
    storm_mapping = df_storms[["sid", "atcf_id", "name"]].drop_duplicates()
    storm_mapping = storm_mapping[
        storm_mapping["sid"].notna() & storm_mapping["atcf_id"].notna()
    ]

    # Merge tracks with storm mapping to get ATCF IDs
    obsv_tracks = obsv_tracks.merge(storm_mapping, on="sid", how="left")

    print(
        f"‚úÖ Loaded {len(obsv_tracks)} track points from {obsv_tracks['sid'].nunique()} storms"
    )
    print(
        f"   Date range: {obsv_tracks['valid_time'].min().date()} to {obsv_tracks['valid_time'].max().date()}"
    )
    print(
        f"   Storms with ATCF mapping: {obsv_tracks['atcf_id'].notna().sum()}/{len(obsv_tracks)}"
    )

    # Rename columns to match what monitoring_utils expects
    obsv_tracks = obsv_tracks.rename(
        columns={"valid_time": "lastUpdate", "wind_speed": "intensity"}
    )

    # Keep original SID for exact matching with pre-computed data
    obsv_tracks["original_sid"] = obsv_tracks["sid"].copy()

except Exception as e:
    print(f"‚ùå Error loading IBTrACS historical data: {e}")
    print("   Falling back to recent NHC data...")
    obsv_tracks = nhc.load_recent_glb_obsv()
    obsv_tracks = obsv_tracks[obsv_tracks["basin"] == "al"]
    obsv_tracks = obsv_tracks.rename(columns={"id": "atcf_id"})
    obsv_tracks["original_sid"] = None  # No SID for recent NHC data
    print(
        f"‚úÖ Loaded {len(obsv_tracks)} recent track points from {obsv_tracks['atcf_id'].nunique()} storms"
    )

print(f"\nüéØ Applying monitoring criteria to identify qualifying storms...")
print(f"   Wind threshold: {THRESHS['obsv']['s']} kt")
print(f"   Must intersect ZMA")

# Create monitor instance to use existing methods
monitor = CubaHurricaneMonitor()
qualifying_storms = []
raster_processor = IMERGRasterProcessor(quantile=0.8)

# monitor._filter_by_zma
# monitor.filter
# Check each storm against monitoring criteria
print(f"\nüîÑ Processing storms to find qualifying ones...")
for i, (sid, group) in enumerate(
    obsv_tracks.groupby("sid")
):  # Group by SID instead of atcf_id
    if i >= 10:  # Limit to first 10 storms for testing
        print(f"   Limiting to first 10 storms for validation...")
        break

    print(f"   Checking {sid}...")

    try:
        # First, apply wind threshold filter (cheap operation)
        max_wind = group["intensity"].max()
        if max_wind < THRESHS["obsv"]["s"]:
            print(
                f"     ‚ùå {sid}: Wind {max_wind:.1f} kt < {THRESHS['obsv']['s']} kt (pre-filter)"
            )
            continue

        print(f"     ‚úÖ {sid}: Passes wind threshold ({max_wind:.1f} kt)")

        # Quick boolean check: does this storm have ANY track points in ZMA? (cheap operation)
        group_clean = monitor._remove_track_duplicates(
            group.sort_values("lastUpdate"), "lastUpdate"
        )

        # Create minimal GeoDataFrame just for ZMA intersection check
        gdf_simple = monitor._create_track_geodataframe(group_clean)
        has_zma_intersection = not monitor._filter_by_zma(gdf_simple).empty

        if not has_zma_intersection:
            print(f"     ‚ùå {sid}: No ZMA intersection (pre-filter)")
            continue

        print(f"     ‚úÖ {sid}: Has ZMA intersection, processing full track...")

        # Now do the expensive interpolation only for storms that pass both filters
        df_interp = monitor._interpolate_track(
            group_clean,
            "lastUpdate",
            ["latitude", "longitude", "intensity", "pressure"],
        )

        # Create full GeoDataFrame and get ZMA-intersecting segments
        gdf = monitor._create_track_geodataframe(df_interp)
        gdf_zma = monitor._filter_by_zma(gdf)

        print(
            f"     ‚úÖ {sid}: QUALIFIES (wind={max_wind:.1f} kt, ZMA intersection)"
        )

        # Store storm info for comparison - use original group data for dates
        storm_start = group["lastUpdate"].min()
        storm_end = group["lastUpdate"].max()

        # Debug output to check date values
        print(
            f"     Debug: storm_start = {storm_start}, storm_end = {storm_end}"
        )
        print(
            f"     Debug: date range = {storm_start.date() if pd.notna(storm_start) else 'NaT'} to {storm_end.date() if pd.notna(storm_end) else 'NaT'}"
        )

        qualifying_storms.append(
            {
                "sid": sid,
                "atcf_id": (
                    group["atcf_id"].iloc[0]
                    if "atcf_id" in group.columns
                    else None
                ),  # Keep ATCF ID for reference
                "storm_start": storm_start,
                "storm_end": storm_end,
                "max_wind": max_wind,
                "min_dist": gdf["distance"].min(),
            }
        )

    except Exception as e:
        print(f"     ‚ö†Ô∏è {sid}: Error processing - {e}")

print(f"\nüìä Summary:")
print(
    f"   Total storms processed: {min(10, obsv_tracks['atcf_id'].nunique())}"
)
print(f"   Qualifying storms: {len(qualifying_storms)}")

if len(qualifying_storms) > 0:
    print(
        f"\nüîÑ Running on-the-fly rainfall calculations for qualifying storms..."
    )

    validation_results = []

    for storm_info in qualifying_storms[:3]:  # Limit to first 3 for speed
        sid = storm_info["sid"]
        atcf_id = storm_info["atcf_id"]
        storm_start = storm_info["storm_start"]
        storm_end = storm_info["storm_end"]

        print(f"   Processing {sid} (ATCF: {atcf_id})...")

        # Extend period by 1 day on each side (same as monitoring_utils)
        analysis_start = storm_start - pd.Timedelta(days=1)
        analysis_end = storm_end + pd.Timedelta(days=1)

        print(f"     Storm period: {storm_start.date()} to {storm_end.date()}")
        print(
            f"     Analysis period: {analysis_start.date()} to {analysis_end.date()}"
        )

        try:
            # Calculate rainfall using monitoring_utils method
            storm_rainfall_df = (
                raster_processor.calculate_rainfall_for_storm_period(
                    analysis_start, analysis_end
                )
            )

            if storm_rainfall_df.empty:
                print(f"     ‚ö†Ô∏è No rainfall data for {sid}")
                continue

            # Get max rainfall for the period (same as monitoring_utils)
            max_rainfall = storm_rainfall_df["roll2_sum"].max()

            print(f"     On-the-fly calculation: {max_rainfall:.2f} mm")

            validation_results.append(
                {
                    "sid": sid,
                    "atcf_id": atcf_id,
                    "storm_start": storm_start,
                    "storm_end": storm_end,
                    "onthefly_max_roll2": max_rainfall,
                    "analysis_start": analysis_start,
                    "analysis_end": analysis_end,
                }
            )

        except Exception as e:
            print(f"     ‚ùå Error calculating rainfall for {sid}: {e}")
            import traceback

            traceback.print_exc()

    # Convert to DataFrame for easier comparison
    df_validation = pd.DataFrame(validation_results)

    if len(df_validation) > 0 and df_precomputed is not None:
        print(f"\nüîç Comparing with pre-computed values...")
        print(f"   On-the-fly results: {len(df_validation)} storms")
        print(f"   Pre-computed results: {len(df_precomputed)} storms")

        # Note: This might need adjustment based on the actual SID format in precomputed data
        print(f"\nüìä Validation results:")
        for _, row in df_validation.iterrows():
            print(
                f"   {row['sid']} (ATCF: {row['atcf_id']}): On-the-fly max roll2 = {row['onthefly_max_roll2']:.2f} mm"
            )

            # Use exact SID matching with pre-computed data
            original_sid = row["sid"]
            potential_match = df_precomputed[
                df_precomputed["sid"] == original_sid
            ]

            if len(potential_match) > 0:
                precomputed_val = potential_match.iloc[0]["q80_roll2"]
                diff = abs(row["onthefly_max_roll2"] - precomputed_val)
                print(
                    f"     Pre-computed q80_roll2 = {precomputed_val:.2f} mm"
                )
                print(f"     Difference = {diff:.2f} mm")
                print(f"     ‚úÖ Exact SID match: {original_sid}")
            else:
                print(f"     ‚ùå No exact SID match found for: {original_sid}")

    print(f"\n‚úÖ Validation complete!")
    print(
        f"üìã Note: This is a basic validation. Full comparison would require:"
    )
    print(f"   - Exact SID matching between datasets")
    print(f"   - Same date ranges and quantile calculations")
    print(f"   - Processing all storms, not just a subset")
else:
    print(f"\n‚ö†Ô∏è  No qualifying storms found in the sample. Try:")
    print(f"   - Increasing the storm limit (currently 10)")
    print(f"   - Using more recent data")
    print(f"   - Checking the wind/ZMA thresholds")
```
